{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Anaconda3.5\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 加载数据集\n",
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'labeled_train': 'train_first.csv',\n",
    "        'test': 'predict_first.csv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('..', 'ccf_data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep=',', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 100000\n",
      "Number of reviews: 30000\n",
      "(130000,)\n"
     ]
    }
   ],
   "source": [
    "df1 = load_dataset('labeled_train').Discuss\n",
    "df2 = load_dataset('test').Discuss\n",
    "df_unlabeled = df1.append(df2)\n",
    "df_unlabeled.index = range(len(df_unlabeled)) # 重新整理下index\n",
    "print(df_unlabeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.读入之前的word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## .model文件\n",
    "model_name = '200features_5minwords_5context.model'\n",
    "model = Word2Vec.load(os.path.join('..', 'ccf_models', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28539, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个model是之前用gensim包生成的word2vec(词频小于5的都过滤掉了)\n",
    "embedding_matrix = model.wv.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.用RNN构建分类器 [这里用的是Tensorflow LSTM模型]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start a graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set RNN parameters\n",
    "epochs = 10\n",
    "batch_size = 5000\n",
    "max_sequence_length = 180\n",
    "rnn_size = 100\n",
    "lstmUnits = 100\n",
    "numClasses = 5\n",
    "embedding_size = 200\n",
    "VALIDATION_SPLIT = 0.75\n",
    "min_word_frequency = 5\n",
    "learning_rate = 0.001\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将gensim生成的word2vec变成字典的形式 比如{'不错':[0.02,0.1,....0.09]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Anaconda3.5\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "word_vec = {}\n",
    "for word in model.wv.vocab:\n",
    "    word_vec[word] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 然后把改字典处理一下，生成tensorflow需要的样子\n",
    "# vocab为词列表（按顺序）\n",
    "# embed为vocab列表中每个词对应的词向量\n",
    "vocab = []\n",
    "embed = []\n",
    "for key,value in word_vec.items():\n",
    "    vocab.append(key)\n",
    "    embed.append(value)\n",
    "vocab = np.asarray(vocab)\n",
    "embedding = np.asarray(embed,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### 这里embedding头部在加一行，和keras不同，这里索引0也得自己定义词向量\n",
    "## 刚开始后面编译的时候会报错，因为embedding_mat是float64而不是float32 !!!!!!\n",
    "embedding = np.row_stack((np.random.uniform(low=-1,high=1,size=200),embedding))\n",
    "embedding = np.asarray(embedding,dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "def split_clean_text2(text, remove_stopwords=False):\n",
    "    text = re.sub(r'。|！|？|～',' ',text.lower().strip())\n",
    "    text = re.sub(r'good|nice|excellent|beautiful','不错',text)\n",
    "#     text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    words = list(jieba.cut(text))\n",
    "    words = [re.sub(r'[^0-9\\u4E00-\\u9FA5]+','',s1).replace(' ','') for s1 in words]\n",
    "    words = [s2 for s2 in words if s2]\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in chn_stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\16121360\\AppData\\Local\\Temp\\11\\jieba.cache\n",
      "Loading model cost 1.097 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 100000\n"
     ]
    }
   ],
   "source": [
    "# texts和labels准备 texts包含测试数据转索引使用\n",
    "# 这里的texts包含了训练和测试数据，每句话转成索引以后，再将训练和测试切分开\n",
    "texts = df_unlabeled.apply(split_clean_text2)\n",
    "df1_score = load_dataset('labeled_train').Score\n",
    "df1_score = df1_score - 1 # tricky\n",
    "labels = np.array(pd.get_dummies(df1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 跑二分类的时候用\n",
    "# labels = np.where(df1_score > 2,1,0)\n",
    "# 跑多分类的时候用\n",
    "labels = np.array(df1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将每句话的单词变成索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change texts into numeric vectors\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length)\n",
    "# text_processed = np.array(list(vocab_processor.fit_transform(texts)))\n",
    "# fit the vocab from gensim\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "# transform inputs\n",
    "text_processed = np.array(list(vocab_processor.transform(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练和测试转成索引以后进行分割\n",
    "x_train_data = text_processed[:100000]\n",
    "x_test = text_processed[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 28540\n",
      "75-25 Train Test split: 75000 -- 25000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split data\n",
    "shuffled_ix = np.random.permutation(np.arange(len(labels)))\n",
    "x_shuffled = x_train_data[shuffled_ix]\n",
    "y_shuffled = labels[shuffled_ix]\n",
    "\n",
    "# Split train/test set\n",
    "ix_cutoff = int(len(y_shuffled)*VALIDATION_SPLIT)\n",
    "x_train, x_val = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
    "y_train, y_val = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "print(\"75-25 Train Test split: {:d} -- {:d}\".format(len(y_train), len(y_val)))\n",
    "del x_shuffled\n",
    "del y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "x_data = tf.placeholder(tf.int32, [None, max_sequence_length])\n",
    "# y_output = tf.placeholder(tf.int32, [None,y_train.shape[1]]) # 这里y_train.shape[1]=5指的是label有5类，是经过one-hot的\n",
    "y_output = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 将embedding加载到tf中\n",
    "embedding_mat = tf.Variable(embedding)\n",
    "embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the BasicRNN cell 二分类\n",
    "#tensorflow change >= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.\n",
    "if tf.__version__[0]>='1':\n",
    "    cell=tf.contrib.rnn.BasicRNNCell(num_units = rnn_size)\n",
    "else:\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units = rnn_size)\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n",
    "output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "\n",
    "# Get output of RNN sequence \n",
    "# 下面两行代码的意思先进行一个dimension的转换，然后一句话最后一个词那里进行输出，比如max_sequence_length=25那么第25个的时候（索引是24）进行输出y_，\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "#-------------------------------------------------------\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1)) # 二分类\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[2]))  # 二分类\n",
    "logits_out = tf.matmul(last, weight) + bias\n",
    "\n",
    "# Loss function 二分类\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output) # logits=float32, labels=int32\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Anaconda3.5\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Define the BasicLSTM cell 多分类\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
    "output, state = tf.nn.dynamic_rnn(lstmCell, embedding_output, dtype=tf.float32)\n",
    "\n",
    "# Get output of RNN sequence \n",
    "# 下面两行代码的意思先进行一个dimension的转换，然后一句话最后一个词那里进行输出，比如max_sequence_length=25那么第25个的时候（索引是24）进行输出y_，\n",
    "output = tf.transpose(output, [1, 0, 2])   # 维度(max_sequence_length,?,rnn_size)\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1) # 维度(?,rnn_size)\n",
    "#-------------------------------------------------------\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))  # 维度 (rnn_size,num_classes)\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))  # 维度 (num_classes,)\n",
    "logits_out = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "# Loss function 多分类\n",
    "# 注: tf.nn.sparse_softmax_cross_entropy_with_logits这个函数输入的labels [5,4,1,2,...,5,4,3]是没有one-hot的，函数本身会进行one-hot\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output) # logits=float32, labels=int32\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, val Loss: 2.2, val Acc: 0.45\n",
      "Epoch: 2, val Loss: 1.9, val Acc: 0.55\n",
      "Epoch: 3, val Loss: 1.4, val Acc: 0.41\n",
      "Epoch: 4, val Loss: 1.5, val Acc: 0.58\n",
      "Epoch: 5, val Loss: 1.2, val Acc: 0.39\n",
      "Epoch: 6, val Loss: 1.1, val Acc: 0.59\n",
      "Epoch: 7, val Loss: 1.0, val Acc: 0.56\n",
      "Epoch: 8, val Loss: 1.0, val Acc: 0.59\n",
      "Epoch: 9, val Loss: 1.0, val Acc: 0.57\n",
      "Epoch: 10, val Loss: 1.0, val Acc: 0.57\n"
     ]
    }
   ],
   "source": [
    "# 开始跑tensorflow\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Shuffle training data\n",
    "    shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "    x_train = x_train[shuffled_ix]\n",
    "    y_train = y_train[shuffled_ix]\n",
    "    num_batches = int(len(x_train)/batch_size) + 1\n",
    "    # TO DO CALCULATE GENERATIONS ExACTLY\n",
    "    for i in range(num_batches):\n",
    "        # Select train data\n",
    "        min_ix = i * batch_size\n",
    "        max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n",
    "        x_train_batch = x_train[min_ix:max_ix]\n",
    "        y_train_batch = y_train[min_ix:max_ix]\n",
    "        \n",
    "        # Run train step\n",
    "        train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}\n",
    "        sess.run(train_step, feed_dict=train_dict)\n",
    "        \n",
    "    # Run loss and accuracy for training\n",
    "    temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n",
    "    train_loss.append(temp_train_loss)\n",
    "    train_accuracy.append(temp_train_acc)\n",
    "    \n",
    "    # Run Eval Step\n",
    "    val_dict = {x_data: x_val, y_output: y_val, dropout_keep_prob:1.0}\n",
    "    temp_val_loss, temp_val_acc = sess.run([loss, accuracy], feed_dict=val_dict)\n",
    "    val_loss.append(temp_val_loss)\n",
    "    val_accuracy.append(temp_val_acc)\n",
    "    print('Epoch: {}, val Loss: {:.2}, val Acc: {:.2}'.format(epoch+1, temp_val_loss, temp_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.根据训练好的tensorflow模型进行预测 以下两种任意一种都可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred1 = sess.run(logits_out,feed_dict={x_data: x_test, dropout_keep_prob:1.0})\n",
    "# pred2 = sess.run(tf.nn.softmax(logits_out),feed_dict={x_data: x_test, dropout_keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = pd.DataFrame(pred1)\n",
    "d = c.apply(np.argmax,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    27340\n",
       "3     2646\n",
       "2       14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 30000\n"
     ]
    }
   ],
   "source": [
    "df_tmp = load_dataset('test')\n",
    "output = pd.DataFrame({'Id':df_tmp.Id,'Score':d})\n",
    "output.to_csv(os.path.join('..', 'ccf_data', 'submit.csv'),sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
