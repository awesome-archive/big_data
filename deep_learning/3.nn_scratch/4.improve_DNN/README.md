## 改善深度神经网络

### [1. He initialization](2_1/1.Initialization.ipynb)

### [2. Regularization](2_1/2.Regularization.ipynb)

 - **L2 Regularization**
 - **Dropout**
 
### [3. Gradient checking](2_1/3.Gradient+Checking.ipynb)

### [4. Optimization methods](2_2/Optimization+methods.ipynb)

 - **Mini-batch gradient descent**: the difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.
 - **Momentum**: reduce oscillations caused by mini-batch
 - **Adam**: one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.

### [5. Tensorflow tutorial](2_3/Tensorflow+Tutorial.ipynb)
