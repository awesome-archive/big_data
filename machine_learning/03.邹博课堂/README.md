## 一级目录

[**1. 机器学习与数据分析**](#机器学习与数据分析)

[**2. 概率论与贝叶斯先验**](#概率论与贝叶斯先验)

[**3. 矩阵和线性代数**](#矩阵和线性代数)

[**8. 回归实践**](#回归实践)

[**9. 决策树和随机森林**](#决策树和随机森林)

[**11. 提升**](#提升)

[**17. 朴素贝叶斯**](#朴素贝叶斯)

---

### 机器学习与数据分析

(1) 凸函数    
(2) 生日悖论    
(3) 本福特定律: 阶乘首数据出现的概率   

### 概率论与贝叶斯先验

(1) 独立和不相关    
独立: p(xy) = p(x)p(y), p(A|B) = p(A), E(XY) = E(X)E(Y), Var(X+Y) = Var(X) + Var(Y) 
不相关: 这里的不相关指的是不线性相关 Cov(X, Y)/Var(X)Var(Y)

举例：x在[-1,1]上均匀分布，y = x^2, 这里x和y显然不独立(因为y和x是有联系的)，但是E(xy) - E(x)E(y) = 0，所以x，y不相关。

(2) 商品推荐      
商品推荐场景中过于聚焦的商品推荐往往会损害用户的购物体验，在有些场景中，系统会通过一定程度的随机性给用户带来发现的惊喜感。

举例：经计算A和B两个商品与当前访问用户的匹配度分别为0.8分和0.2分，系统将随机为A生成一个均匀分布于0到0.8的最终得分，为B生成一个均匀分布于0到0.2的最终得分，所以B还是有可能推荐给用户的。

(3) 指数分布的无记忆性    
如果一个随机变量呈指数分布，当s，t>=0时有： 
P(x > s + t|x > s) = P(x > t)

(4) 切比雪夫不等式    
设随机变量X的期望为μ，方差为σ^2，对于任意正数ε有： 
P{|X-μ|>=ε} <= σ^2/ε^2 
该不等式进一步说明了方差的含义 
该不等式可证明大数定理

(5) 大数定理   

### 矩阵和线性代数

(1) A = U∑V,其中∑是A(T)A的特征值   
(2) 特征值各不相等时，则特征向量正交   
(3) Ax对x求导得到的是A的转置  

### 回归实践

(1) logistic回归的损失 yi属于{-1，1} 
loss(y,yhat) = -l(theta) 
损失函数即负的最大对数似然函数    
--扩展：    
loss(y,yhat) = 加总yi*ln(yihat) 也可以当损失函数（即交叉熵）

### 决策树和随机森林

(1) 条件熵的定义：H(Y|X)=H(X,Y)-H(X)    
根据互信息定义展开得到：H(Y|X)=H(Y)-I(X,Y)    
所以有些文献将I(X,Y)=H(Y)-H(Y|X)作为互信息的定义式子    
I(X,Y)是KL散度:    
I(X;Y) = DKL(p(x,y)||p(x)p(y))    
互信息表示两个变量X和Y之间是否有关系，以及关系的强弱，趋近于0表示独立，越大相关性越强    

### 提升

(1) 如何进行子树的划分：    
ID3/C4.5/CART，使用贪心法

### 朴素贝叶斯

(1) P(theta|x)后验 = P(x|theta)P(theta) / p(x) --> P(x|theta)似然P(theta)先验    
在贝叶斯概率理论中，如果后验概率P(theta|x)和先验概率P(theta)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布



